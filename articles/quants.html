<!doctype html>
<html lang="en">
    <head>
        <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-DKFJQDBVVF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-DKFJQDBVVF');
</script>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta http-equiv="X-UA-Compatible" content="ie=edge" />
        <title>Getting better models by dropping floating point sizes and increasing parameters</title>
        <link
            rel="stylesheet"
            href="../style.css"
            type="text/css"
            media="all"
        />
        <link
            rel="shortcut icon"
            href="../images/Favicon.png"
            type="image/x-icon"
        />
        <link
            rel="stylesheet"
            href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/monokai.min.css"
        />
        <link
            rel="stylesheet"
            href="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css"
            integrity="sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv"
            crossorigin="anonymous"
        />
    </head>

    <body>
          <!-- The loading of KaTeX is deferred to speed up page rendering -->
        <script
            src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.js"
            integrity="sha384-483A6DwYfKeDa0Q52fJmxFXkcPCFfnXMoXblOkJ4JcA8zATN6Tm78UNL72AKk+0O"
            crossorigin="anonymous"
        ></script>

        <!-- To automatically render math in text elements, include the auto-render extension: -->
        <script
            defer
            src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/contrib/auto-render.min.js"
            integrity="sha384-yACMu8JWxKzSp/C1YV86pzGiQ/l1YUfE8oPuahJQxzehAjEt2GiQuy/BIvl9KyeF"
            crossorigin="anonymous"
            onload="renderMathInElement(document.body);"
        ></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

        <!-- and it's easy to individually load additional languages -->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/go.min.js"></script>

        <script>
            hljs.highlightAll();
        </script>
     <h1 id="getting-better-models-by-dropping-floating-point-sizes-and-increasing-parameters">Getting better models by dropping floating point sizes and increasing parameters</h1>
<p>Another quick post, but I&#39;m continuing some work from a couple months ago, where I trained several neural nets in diff datatypes, and it was only fp16/fp32/fp64, and still is (would like to do more, but need to learn how to make datatype code). I decided to try the approach where the weights are space equivalent, or same amount of GB.</p>
<h2 id="original-conclusion">Original conclusion</h2>
<p>From my <a href="https://github.com/VatsaDev/floatloss">original work</a>, (inspired by <a href="https://x.com/_VatsaDev_/status/1796179027708854417">@kosenjuu</a>), the final graph looked like this: </p>
<p><img src="../images/fpgraph.png" alt="loss"></p>
<p>Loss on the same model with diff floating points does nothing, but I think more floating points with half the precision each time will still work better</p>
<h2 id="training-runs">Training runs</h2>
<p>3 configs,</p>
<ul>
<li>4 layers (fp64)</li>
<li>8 layers (fp32)</li>
<li>16 layers (fp16)</li>
</ul>
<table>
<thead>
<tr>
<th>config</th>
<th>dtype</th>
<th>layers</th>
<th>(train time)/step</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>fp64</td>
<td>4</td>
<td>170ms</td>
</tr>
<tr>
<td>2</td>
<td>fp32</td>
<td>8</td>
<td>430ms</td>
</tr>
<tr>
<td>3</td>
<td>fp16</td>
<td>16</td>
<td>3500ms</td>
</tr>
</tbody>
</table>
<p>the overall loss graph looks like this:</p>
<p><img src="../images/fsizeQualgraph.png" alt="losses"></p>
<h2 id="q1-q2-q4-and-beyond">q1/q2/q4 and beyond</h2>
<ul>
<li>need to figure out how people make custom datatypes in pytorch, then get to the lowest values, and see how it goes</li>
<li>they also need to be gpu-useable, I tried the float8 datatype, was unsuccessful</li>
</ul>
    </body>
</html>
